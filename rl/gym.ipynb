{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Discrete(6)\n",
      "Action space: Tuple(Discrete(2), Discrete(2), Discrete(5))\n",
      "---\n",
      "\n",
      "Total length of input instance: 2, step: 0\n",
      "==========================================\n",
      "Observation Tape    :   \u001b[42mB\u001b[0mA  \n",
      "Output Tape         :   \n",
      "Targets             :   BA  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BEGIN\n",
      "\n",
      "Total length of input instance: 2, step: 1\n",
      "==========================================\n",
      "Observation Tape    :  \u001b[42m \u001b[0mBA  \n",
      "Output Tape         :   \n",
      "Targets             :   BA  \n",
      "\n",
      "Current reward      :   0.000\n",
      "Cumulative reward   :   0.000\n",
      "Action              :   Tuple(move over input: left,\n",
      "                              write to the output tape: False,\n",
      "                              prediction: A)\n",
      "Total length of input instance: 2, step: 2\n",
      "==========================================\n",
      "Observation Tape    :   \u001b[42mB\u001b[0mA  \n",
      "Output Tape         :   \n",
      "Targets             :   BA  \n",
      "\n",
      "Current reward      :   0.000\n",
      "Cumulative reward   :   0.000\n",
      "Action              :   Tuple(move over input: right,\n",
      "                              write to the output tape: False,\n",
      "                              prediction: A)\n",
      "Total length of input instance: 2, step: 3\n",
      "==========================================\n",
      "Observation Tape    :   B\u001b[42mA\u001b[0m  \n",
      "Output Tape         :   \u001b[42mB\u001b[0m\n",
      "Targets             :   BA  \n",
      "\n",
      "Current reward      :   1.000\n",
      "Cumulative reward   :   1.000\n",
      "Action              :   Tuple(move over input: right,\n",
      "                              write to the output tape: True,\n",
      "                              prediction: B)\n",
      "Total length of input instance: 2, step: 4\n",
      "==========================================\n",
      "Observation Tape    :   BA\u001b[42m \u001b[0m \n",
      "Output Tape         :   B\u001b[42mA\u001b[0m\n",
      "Targets             :   BA  \n",
      "\n",
      "Current reward      :   1.000\n",
      "Cumulative reward   :   2.000\n",
      "Action              :   Tuple(move over input: right,\n",
      "                              write to the output tape: True,\n",
      "                              prediction: A)\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('Copy-v0')\n",
    "\n",
    "m = {}\n",
    "\n",
    "print('Observation space:', env.observation_space)\n",
    "print('Action space:', env.action_space)\n",
    "print('---')\n",
    "print()\n",
    "\n",
    "\"\"\"\n",
    "idx: 0\n",
    "    0: flyt hoved venstre\n",
    "    1: flyt hoved højre\n",
    "idx: 1\n",
    "    0: læs\n",
    "    1: skriv\n",
    "idx: 2\n",
    "    0: skriv 'A'\n",
    "    1: skriv 'B'\n",
    "    2: skriv 'C'\n",
    "    3: skriv 'D'\n",
    "    4: skriv 'E'\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Algorithm:\n",
    "læs og flyt til venstre indtil observation = 5\n",
    "læs og flyt til højre\n",
    "hvis 5: done\n",
    "ellers: skriv obs\n",
    "\"\"\"\n",
    "STEP_LEFT_READ = (0,0,0)\n",
    "STEP_RIGHT_READ = (1,0,0)\n",
    "STEP_RIGHT_WRITE = (1,1,)\n",
    "\n",
    "def act_render(env, action):\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    return observation, reward, done, info\n",
    "\n",
    "env.reset()\n",
    "env.render()\n",
    "print('BEGIN')\n",
    "print()\n",
    "observation, reward, done, info = act_render(env, STEP_LEFT_READ)\n",
    "while observation < 5: \n",
    "    observation, reward, done, info = act_render(env, STEP_LEFT_READ)\n",
    "observation, reward, done, info = act_render(env, STEP_RIGHT_READ)\n",
    "while not done:\n",
    "    observation, reward, done, info = act_render(env, STEP_RIGHT_WRITE + (observation,))\n",
    "\n",
    "print('END')\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration# 2357.\n",
      "[[1.   1.   1.   1.   1.   1.   1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.  ]\n",
      " [1.   0.98 0.93 0.   0.86 0.95 0.98 1.  ]\n",
      " [1.   0.93 0.8  0.47 0.62 0.   0.94 1.  ]\n",
      " [1.   0.83 0.54 0.   0.54 0.61 0.85 1.  ]\n",
      " [1.   0.   0.   0.17 0.38 0.44 0.   1.  ]\n",
      " [1.   0.   0.19 0.12 0.   0.33 0.   1.  ]\n",
      " [1.   0.73 0.46 0.   0.28 0.55 0.78 0.  ]]\n",
      "\n",
      "LEFT = 0\n",
      "DOWN = 1\n",
      "RIGHT = 2\n",
      "UP = 3    \n",
      "    \n",
      "[[1 2 2 1 2 2 2 2]\n",
      " [3 3 3 3 3 3 3 2]\n",
      " [0 0 0 0 2 3 3 2]\n",
      " [0 0 0 1 0 0 2 2]\n",
      " [0 3 0 0 2 1 3 2]\n",
      " [0 0 0 1 3 0 0 2]\n",
      " [0 0 1 0 0 0 0 2]\n",
      " [0 1 0 0 1 2 1 0]]\n",
      "Policy average score =  0.876\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Value-Itertion.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "RENDER=False\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "    if render:\n",
    "        env.render()\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma = gamma, render = RENDER)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.env.nS)\n",
    "    for s in range(env.env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for p, s_, r, _ in env.env.P[s][a]:\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Value-iteration algorithm \"\"\"\n",
    "    v = np.zeros(env.env.nS)  # initialize value-function to zeros\n",
    "    max_iterations = 100000\n",
    "    eps = 1e-10\n",
    "    for i in range(max_iterations):\n",
    "        #print(np.ceil(v.reshape([8,8]))); print()\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.env.nS):\n",
    "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.env.P[s][a]]) for a in range(env.env.nA)] \n",
    "            v[s] = max(q_sa)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
    "            print ('Value-iteration converged at iteration %d.' %(i+1))\n",
    "            break\n",
    "    return v\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    gamma = 1.0\n",
    "    env = gym.make(env_name)\n",
    "    optimal_v = value_iteration(env, gamma);\n",
    "    print(np.around(optimal_v.reshape((8,8)), decimals=2))\n",
    "    policy = extract_policy(optimal_v, gamma)\n",
    "    print(\"\"\"\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3    \n",
    "    \"\"\")\n",
    "    print(policy.reshape((8,8)).astype(int))\n",
    "    policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
    "    print('Policy average score = ', policy_score)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at iteration 9.\n",
      "Average scores =  0.888\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Policy iteration.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.env.nS)\n",
    "    for s in range(env.env.nS):\n",
    "        q_sa = np.zeros(env.env.nA)\n",
    "        for a in range(env.env.nA):\n",
    "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.env.P[s][a]])\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "def compute_policy_v(env, policy, gamma=1.0):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "    policy = np.random.choice(env.env.nA, size=(env.env.nS))  # initialize a random policy\n",
    "    max_iterations = 200000\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
    "        new_policy = extract_policy(old_policy_v, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at iteration %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy\n",
    "\n",
    "\n",
    "env_name  = 'FrozenLake8x8-v0'\n",
    "env = gym.make(env_name)\n",
    "optimal_policy = policy_iteration(env, gamma = 1.0)\n",
    "scores = evaluate_policy(env, optimal_policy, gamma = 1.0, n=1000)\n",
    "print('Average scores = ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "-0.5891279887498433 0.0\n",
      "Box(2,)\n",
      "Discrete(3)\n",
      "Begin sim\n",
      "Total reward: -114.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hand-crafted algorithm for MountainCar.\n",
    "Steer in direction of velocity if car is in motion, \n",
    "else neutral.\n",
    "May waste energy climbing higher than necessary on left bank \n",
    "\"\"\"\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from time import sleep\n",
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "left, neutral, right = 0, 1, 2\n",
    "\n",
    "pos, vel = env.reset()\n",
    "print(pos, vel)\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print('Begin sim')\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "    if vel < 0: action = left\n",
    "    elif vel > 0: action = right\n",
    "    else: action = neutral\n",
    "    (pos, vel), r, done, _ = env.step(action)\n",
    "    total_reward += r\n",
    "    sleep(.1)\n",
    "    \n",
    "print('Total reward:', total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "----- using Q Learning -----\n",
      "Iteration #1 -- Total reward = -200.\n",
      "Iteration #101 -- Total reward = -200.\n",
      "Iteration #201 -- Total reward = -200.\n",
      "Iteration #301 -- Total reward = -200.\n",
      "Iteration #401 -- Total reward = -200.\n",
      "Iteration #501 -- Total reward = -200.\n",
      "Iteration #601 -- Total reward = -200.\n",
      "Iteration #701 -- Total reward = -200.\n",
      "Iteration #801 -- Total reward = -200.\n",
      "Iteration #901 -- Total reward = -200.\n",
      "Iteration #1001 -- Total reward = -200.\n",
      "Iteration #1101 -- Total reward = -200.\n",
      "Iteration #1201 -- Total reward = -200.\n",
      "Iteration #1301 -- Total reward = -200.\n",
      "Iteration #1401 -- Total reward = -200.\n",
      "Iteration #1501 -- Total reward = -200.\n",
      "Iteration #1601 -- Total reward = -200.\n",
      "Iteration #1701 -- Total reward = -200.\n",
      "Iteration #1801 -- Total reward = -200.\n",
      "Iteration #1901 -- Total reward = -200.\n",
      "Iteration #2001 -- Total reward = -200.\n",
      "Iteration #2101 -- Total reward = -200.\n",
      "Iteration #2201 -- Total reward = -200.\n",
      "Iteration #2301 -- Total reward = -200.\n",
      "Iteration #2401 -- Total reward = -200.\n",
      "Iteration #2501 -- Total reward = -200.\n",
      "Iteration #2601 -- Total reward = -200.\n",
      "Iteration #2701 -- Total reward = -200.\n",
      "Iteration #2801 -- Total reward = -200.\n",
      "Iteration #2901 -- Total reward = -200.\n",
      "Iteration #3001 -- Total reward = -200.\n",
      "Iteration #3101 -- Total reward = -200.\n",
      "Iteration #3201 -- Total reward = -200.\n",
      "Iteration #3301 -- Total reward = -200.\n",
      "Iteration #3401 -- Total reward = -200.\n",
      "Iteration #3501 -- Total reward = -200.\n",
      "Iteration #3601 -- Total reward = -200.\n",
      "Iteration #3701 -- Total reward = -200.\n",
      "Iteration #3801 -- Total reward = -200.\n",
      "Iteration #3901 -- Total reward = -200.\n",
      "Iteration #4001 -- Total reward = -200.\n",
      "Iteration #4101 -- Total reward = -200.\n",
      "Iteration #4201 -- Total reward = -200.\n",
      "Iteration #4301 -- Total reward = -200.\n",
      "Iteration #4401 -- Total reward = -200.\n",
      "Iteration #4501 -- Total reward = -200.\n",
      "Iteration #4601 -- Total reward = -200.\n",
      "Iteration #4701 -- Total reward = -200.\n",
      "Iteration #4801 -- Total reward = -200.\n",
      "Iteration #4901 -- Total reward = -200.\n",
      "Iteration #5001 -- Total reward = -200.\n",
      "Iteration #5101 -- Total reward = -200.\n",
      "Iteration #5201 -- Total reward = -200.\n",
      "Iteration #5301 -- Total reward = -200.\n",
      "Iteration #5401 -- Total reward = -200.\n",
      "Iteration #5501 -- Total reward = -200.\n",
      "Iteration #5601 -- Total reward = -200.\n",
      "Iteration #5701 -- Total reward = -200.\n",
      "Iteration #5801 -- Total reward = -200.\n",
      "Iteration #5901 -- Total reward = -200.\n",
      "Iteration #6001 -- Total reward = -200.\n",
      "Iteration #6101 -- Total reward = -200.\n",
      "Iteration #6201 -- Total reward = -200.\n",
      "Iteration #6301 -- Total reward = -200.\n",
      "Iteration #6401 -- Total reward = -200.\n",
      "Iteration #6501 -- Total reward = -200.\n",
      "Iteration #6601 -- Total reward = -200.\n",
      "Iteration #6701 -- Total reward = -200.\n",
      "Iteration #6801 -- Total reward = -200.\n",
      "Iteration #6901 -- Total reward = -200.\n",
      "Iteration #7001 -- Total reward = -200.\n",
      "Iteration #7101 -- Total reward = -200.\n",
      "Iteration #7201 -- Total reward = -200.\n",
      "Iteration #7301 -- Total reward = -200.\n",
      "Iteration #7401 -- Total reward = -200.\n",
      "Iteration #7501 -- Total reward = -200.\n",
      "Iteration #7601 -- Total reward = -200.\n",
      "Iteration #7701 -- Total reward = -200.\n",
      "Iteration #7801 -- Total reward = -200.\n",
      "Iteration #7901 -- Total reward = -200.\n",
      "Iteration #8001 -- Total reward = -198.\n",
      "Iteration #8101 -- Total reward = -200.\n",
      "Iteration #8201 -- Total reward = -200.\n",
      "Iteration #8301 -- Total reward = -200.\n",
      "Iteration #8401 -- Total reward = -200.\n",
      "Iteration #8501 -- Total reward = -200.\n",
      "Iteration #8601 -- Total reward = -200.\n",
      "Iteration #8701 -- Total reward = -200.\n",
      "Iteration #8801 -- Total reward = -200.\n",
      "Iteration #8901 -- Total reward = -200.\n",
      "Iteration #9001 -- Total reward = -200.\n",
      "Iteration #9101 -- Total reward = -200.\n",
      "Iteration #9201 -- Total reward = -200.\n",
      "Iteration #9301 -- Total reward = -200.\n",
      "Iteration #9401 -- Total reward = -200.\n",
      "Iteration #9501 -- Total reward = -200.\n",
      "Iteration #9601 -- Total reward = -200.\n",
      "Iteration #9701 -- Total reward = -200.\n",
      "Iteration #9801 -- Total reward = -200.\n",
      "Iteration #9901 -- Total reward = -200.\n",
      "Average score of solution =  -129.96\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q-Learning example using OpenAI gym MountainCar enviornment\n",
    "Author: Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "n_states = 40\n",
    "iter_max = 10000\n",
    "\n",
    "initial_lr = 1.0 # Learning rate\n",
    "min_lr = 0.003\n",
    "gamma = 1.0\n",
    "t_max = 10000\n",
    "eps = 0.02\n",
    "\n",
    "def run_episode(env, policy=None, render=False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    for _ in range(t_max):\n",
    "        if render:\n",
    "            env.render()\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            a,b = obs_to_state(env, obs)\n",
    "            action = policy[a][b]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += gamma ** step_idx * reward\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def obs_to_state(env, obs):\n",
    "    \"\"\" Maps an observation to state \"\"\"\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    env_dx = (env_high - env_low) / n_states\n",
    "    a = int((obs[0] - env_low[0])/env_dx[0])\n",
    "    b = int((obs[1] - env_low[1])/env_dx[1])\n",
    "    return a, b\n",
    "\n",
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "print ('----- using Q Learning -----')\n",
    "q_table = np.zeros((n_states, n_states, 3))\n",
    "for i in range(iter_max):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    ## eta: learning rate is decreased at each step\n",
    "    eta = max(min_lr, initial_lr * (0.85 ** (i//100)))\n",
    "    for j in range(t_max):\n",
    "        a, b = obs_to_state(env, obs)\n",
    "        if np.random.uniform(0, 1) < eps:\n",
    "            action = np.random.choice(env.action_space.n)\n",
    "        else:\n",
    "            logits = q_table[a][b]\n",
    "            logits_exp = np.exp(logits)\n",
    "            probs = logits_exp / np.sum(logits_exp)\n",
    "            action = np.random.choice(env.action_space.n, p=probs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        # update q table\n",
    "        a_, b_ = obs_to_state(env, obs)\n",
    "        q_table[a][b][action] = q_table[a][b][action] + eta * (reward + gamma *  np.max(q_table[a_][b_]) - q_table[a][b][action])\n",
    "        if done:\n",
    "            break\n",
    "    if i % 100 == 0:\n",
    "        print('Iteration #%d -- Total reward = %d.' %(i+1, total_reward))\n",
    "solution_policy = np.argmax(q_table, axis=2)\n",
    "solution_policy_scores = [run_episode(env, solution_policy, False) for _ in range(100)]\n",
    "print(\"Average score of solution = \", np.mean(solution_policy_scores))\n",
    "# Animate it\n",
    "run_episode(env, solution_policy, True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "----- using Q Learning -----\n",
      "Iteration #1 -- Total reward = -200.\n",
      "Iteration #101 -- Total reward = -200.\n",
      "Iteration #201 -- Total reward = -200.\n",
      "Iteration #301 -- Total reward = -200.\n",
      "Iteration #401 -- Total reward = -200.\n",
      "Iteration #501 -- Total reward = -200.\n",
      "Iteration #601 -- Total reward = -200.\n",
      "Iteration #701 -- Total reward = -200.\n",
      "Iteration #801 -- Total reward = -200.\n",
      "Iteration #901 -- Total reward = -200.\n",
      "Iteration #1001 -- Total reward = -200.\n",
      "Iteration #1101 -- Total reward = -200.\n",
      "Iteration #1201 -- Total reward = -200.\n",
      "Iteration #1301 -- Total reward = -200.\n",
      "Iteration #1401 -- Total reward = -200.\n",
      "Iteration #1501 -- Total reward = -200.\n",
      "Iteration #1601 -- Total reward = -200.\n",
      "Iteration #1701 -- Total reward = -200.\n",
      "Iteration #1801 -- Total reward = -200.\n",
      "Iteration #1901 -- Total reward = -200.\n",
      "Iteration #2001 -- Total reward = -200.\n",
      "Iteration #2101 -- Total reward = -200.\n",
      "Iteration #2201 -- Total reward = -200.\n",
      "Iteration #2301 -- Total reward = -200.\n",
      "Iteration #2401 -- Total reward = -200.\n",
      "Iteration #2501 -- Total reward = -200.\n",
      "Iteration #2601 -- Total reward = -200.\n",
      "Iteration #2701 -- Total reward = -200.\n",
      "Iteration #2801 -- Total reward = -200.\n",
      "Iteration #2901 -- Total reward = -200.\n",
      "Iteration #3001 -- Total reward = -200.\n",
      "Iteration #3101 -- Total reward = -200.\n",
      "Iteration #3201 -- Total reward = -200.\n",
      "Iteration #3301 -- Total reward = -200.\n",
      "Iteration #3401 -- Total reward = -200.\n",
      "Iteration #3501 -- Total reward = -200.\n",
      "Iteration #3601 -- Total reward = -200.\n",
      "Iteration #3701 -- Total reward = -200.\n",
      "Iteration #3801 -- Total reward = -200.\n",
      "Iteration #3901 -- Total reward = -200.\n",
      "Iteration #4001 -- Total reward = -200.\n",
      "Iteration #4101 -- Total reward = -200.\n",
      "Iteration #4201 -- Total reward = -200.\n",
      "Iteration #4301 -- Total reward = -200.\n",
      "Iteration #4401 -- Total reward = -200.\n",
      "Iteration #4501 -- Total reward = -200.\n",
      "Iteration #4601 -- Total reward = -200.\n",
      "Iteration #4701 -- Total reward = -200.\n",
      "Iteration #4801 -- Total reward = -200.\n",
      "Iteration #4901 -- Total reward = -200.\n",
      "Average score of solution =  -139.92\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q-Learning for MountainCar\n",
    "Altered reward = velocity, \n",
    "Author: Pimin Konstantin Kefaloukos\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "n_states = 40\n",
    "iter_max = 5000\n",
    "\n",
    "initial_lr = 1.0 # Learning rate\n",
    "min_lr = 0.003\n",
    "gamma = 1.0\n",
    "t_max = 10000\n",
    "eps = 0.02\n",
    "\n",
    "def run_episode(env, policy=None, render=False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    for _ in range(t_max):\n",
    "        if render:\n",
    "            env.render()\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            a,b = obs_to_state(env, obs)\n",
    "            action = policy[a][b]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += gamma ** step_idx * reward\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def obs_to_state(env, obs):\n",
    "    \"\"\" Maps an observation to state \"\"\"\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    env_dx = (env_high - env_low) / n_states\n",
    "    a = int((obs[0] - env_low[0])/env_dx[0])\n",
    "    b = int((obs[1] - env_low[1])/env_dx[1])\n",
    "    return a, b\n",
    "\n",
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "print ('----- using Q Learning -----')\n",
    "q_table = np.zeros((n_states, n_states, 3))\n",
    "for i in range(iter_max):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    ## eta: learning rate is decreased at each step\n",
    "    eta = max(min_lr, initial_lr * (0.85 ** (i//100)))\n",
    "    for j in range(t_max):\n",
    "        a, b = obs_to_state(env, obs)\n",
    "        if np.random.uniform(0, 1) < eps:\n",
    "            action = np.random.choice(env.action_space.n)\n",
    "        else:\n",
    "            logits = q_table[a][b]\n",
    "            logits_exp = np.exp(logits)\n",
    "            probs = logits_exp / np.sum(logits_exp)\n",
    "            action = np.random.choice(env.action_space.n, p=probs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        #reward = abs(obs[1])\n",
    "        #print(reward)\n",
    "        total_reward += reward\n",
    "        # update q table\n",
    "        a_, b_ = obs_to_state(env, obs)\n",
    "        q_table[a][b][action] = q_table[a][b][action] + eta * (reward + gamma *  np.max(q_table[a_][b_]) - q_table[a][b][action])\n",
    "        if done:\n",
    "            break\n",
    "    if i % 100 == 0:\n",
    "        print('Iteration #%d -- Total reward = %d.' %(i+1, total_reward))\n",
    "solution_policy = np.argmax(q_table, axis=2)\n",
    "solution_policy_scores = [run_episode(env, solution_policy, False) for _ in range(100)]\n",
    "print(\"Average score of solution = \", np.mean(solution_policy_scores))\n",
    "# Animate it\n",
    "run_episode(env, solution_policy, True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
