{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now the rich cherry, whose sleek wood,\n",
      "and top with silver petals traced\n",
      "like a strict box its gems encased,\n",
      "has spilt from out that cunning lid,\n",
      "all in an innocent green round,\n",
      "those melting rubies which it hid;\n",
      "with moss ripe-strawberry-encrusted,\n",
      "so birds get half, and minds lapse merry\n",
      "to taste that deep-red, lark’s-bite berry,\n",
      "and blackcap bloom is yellow-dusted.\n",
      "the wren that thieved it in the eaves\n",
      "a trailer of the rose could catch\n",
      "to her poor droopy sloven thatch,\n",
      "and side by side with the wren’s brood—\n",
      "o lovely time of beggar’s luck—\n",
      "opens the quaint and hairy bud;\n",
      "and full and golden is the yield\n",
      "of cows that never have to house,\n",
      "but all night nibble under boughs,\n",
      "or cool their sides in the moist field.\n",
      "into the rooms flow meadow airs,\n",
      "the warm farm baking smell’s blown round.\n",
      "inside and out, and sky and ground\n",
      "are much the same; the wishing star,\n",
      "hesperus, kind and early born,\n",
      "is risen only finger-far;\n",
      "all stars stand close in summer air,\n",
      "and tremble, and look mild as amber;\n",
      "when wicks are lighted in the chamber,\n",
      "they are like stars which settled there.\n",
      "now straightening from the flowery hay,\n",
      "down the still light the mowers look,\n",
      "or turn, because their dreaming shook,\n",
      "and they waked half to other days,\n",
      "when left alone in the yellow stubble\n",
      "the rusty-coated mare would graze.\n",
      "yet thick the lazy dreams are born,\n",
      "another thought can come to mind,\n",
      "but like the shivering of the wind,\n",
      "morning and evening in the corn.\n",
      "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\x97', '\\xa0', '³', 'à', 'á', 'â', 'æ', 'è', 'é', 'í', 'ó', 'ö', 'ø', 'ÿ', 'ɐ', 'ˀ', 'ˈ', 'ː', '̯', '–', '—', '‘', '’', '“', '”', '\\ufeff']\n",
      "Total Characters:  170427\n",
      "Total distinct:  83\n",
      "Total Patterns:  167397\n",
      "Epoch 1/200\n",
      "167397/167397 [==============================] - 195s 1ms/step - loss: 2.4965\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.49651, saving model to saved/weights-001.hdf5\n",
      "Epoch 2/200\n",
      "167397/167397 [==============================] - 193s 1ms/step - loss: 2.1239\n",
      "\n",
      "Epoch 00002: loss improved from 2.49651 to 2.12393, saving model to saved/weights-002.hdf5\n",
      "Epoch 3/200\n",
      "167397/167397 [==============================] - 195s 1ms/step - loss: 1.9991\n",
      "\n",
      "Epoch 00003: loss improved from 2.12393 to 1.99912, saving model to saved/weights-003.hdf5\n",
      "Epoch 4/200\n",
      "167397/167397 [==============================] - 195s 1ms/step - loss: 1.9152\n",
      "\n",
      "Epoch 00004: loss improved from 1.99912 to 1.91520, saving model to saved/weights-004.hdf5\n",
      "Epoch 5/200\n",
      "167397/167397 [==============================] - 193s 1ms/step - loss: 1.8528\n",
      "\n",
      "Epoch 00005: loss improved from 1.91520 to 1.85285, saving model to saved/weights-005.hdf5\n",
      "Epoch 6/200\n",
      "167397/167397 [==============================] - 193s 1ms/step - loss: 1.7957\n",
      "\n",
      "Epoch 00006: loss improved from 1.85285 to 1.79572, saving model to saved/weights-006.hdf5\n",
      "Epoch 7/200\n",
      "167397/167397 [==============================] - 193s 1ms/step - loss: 1.7473\n",
      "\n",
      "Epoch 00007: loss improved from 1.79572 to 1.74726, saving model to saved/weights-007.hdf5\n",
      "Epoch 8/200\n",
      "167397/167397 [==============================] - 194s 1ms/step - loss: 1.7032\n",
      "\n",
      "Epoch 00008: loss improved from 1.74726 to 1.70318, saving model to saved/weights-008.hdf5\n",
      "Epoch 9/200\n",
      "167397/167397 [==============================] - 194s 1ms/step - loss: 1.6636\n",
      "\n",
      "Epoch 00009: loss improved from 1.70318 to 1.66359, saving model to saved/weights-009.hdf5\n",
      "Epoch 10/200\n",
      "167397/167397 [==============================] - 2296s 14ms/step - loss: 1.6248\n",
      "\n",
      "Epoch 00010: loss improved from 1.66359 to 1.62485, saving model to saved/weights-010.hdf5\n",
      "Epoch 11/200\n",
      "167397/167397 [==============================] - 188s 1ms/step - loss: 1.5895\n",
      "\n",
      "Epoch 00011: loss improved from 1.62485 to 1.58949, saving model to saved/weights-011.hdf5\n",
      "Epoch 12/200\n",
      "167397/167397 [==============================] - 194s 1ms/step - loss: 1.5568\n",
      "\n",
      "Epoch 00012: loss improved from 1.58949 to 1.55682, saving model to saved/weights-012.hdf5\n",
      "Epoch 13/200\n",
      "167397/167397 [==============================] - 197s 1ms/step - loss: 1.5266\n",
      "\n",
      "Epoch 00013: loss improved from 1.55682 to 1.52655, saving model to saved/weights-013.hdf5\n",
      "Epoch 14/200\n",
      "167397/167397 [==============================] - 194s 1ms/step - loss: 1.4990\n",
      "\n",
      "Epoch 00014: loss improved from 1.52655 to 1.49903, saving model to saved/weights-014.hdf5\n",
      "Epoch 15/200\n",
      "167397/167397 [==============================] - 194s 1ms/step - loss: 1.4718\n",
      "\n",
      "Epoch 00015: loss improved from 1.49903 to 1.47177, saving model to saved/weights-015.hdf5\n",
      "Epoch 16/200\n",
      "167397/167397 [==============================] - 188s 1ms/step - loss: 1.4461\n",
      "\n",
      "Epoch 00016: loss improved from 1.47177 to 1.44609, saving model to saved/weights-016.hdf5\n",
      "Epoch 17/200\n",
      "167397/167397 [==============================] - 189s 1ms/step - loss: 1.4231\n",
      "\n",
      "Epoch 00017: loss improved from 1.44609 to 1.42306, saving model to saved/weights-017.hdf5\n",
      "Epoch 18/200\n",
      "167397/167397 [==============================] - 191s 1ms/step - loss: 1.3965\n",
      "\n",
      "Epoch 00018: loss improved from 1.42306 to 1.39647, saving model to saved/weights-018.hdf5\n",
      "Epoch 19/200\n",
      "167397/167397 [==============================] - 197s 1ms/step - loss: 1.3737\n",
      "\n",
      "Epoch 00019: loss improved from 1.39647 to 1.37367, saving model to saved/weights-019.hdf5\n",
      "Epoch 20/200\n",
      "167397/167397 [==============================] - 191s 1ms/step - loss: 1.3564\n",
      "\n",
      "Epoch 00020: loss improved from 1.37367 to 1.35638, saving model to saved/weights-020.hdf5\n",
      "Epoch 21/200\n",
      "167397/167397 [==============================] - 190s 1ms/step - loss: 1.3378\n",
      "\n",
      "Epoch 00021: loss improved from 1.35638 to 1.33781, saving model to saved/weights-021.hdf5\n",
      "Epoch 22/200\n",
      "167397/167397 [==============================] - 190s 1ms/step - loss: 1.3185\n",
      "\n",
      "Epoch 00022: loss improved from 1.33781 to 1.31851, saving model to saved/weights-022.hdf5\n",
      "Epoch 23/200\n",
      "167397/167397 [==============================] - 191s 1ms/step - loss: 1.3016\n",
      "\n",
      "Epoch 00023: loss improved from 1.31851 to 1.30163, saving model to saved/weights-023.hdf5\n",
      "Epoch 24/200\n",
      "167397/167397 [==============================] - 191s 1ms/step - loss: 1.2846\n",
      "\n",
      "Epoch 00024: loss improved from 1.30163 to 1.28458, saving model to saved/weights-024.hdf5\n",
      "Epoch 25/200\n",
      "167397/167397 [==============================] - 192s 1ms/step - loss: 1.2697\n",
      "\n",
      "Epoch 00025: loss improved from 1.28458 to 1.26971, saving model to saved/weights-025.hdf5\n",
      "Epoch 26/200\n",
      "167397/167397 [==============================] - 192s 1ms/step - loss: 1.2556\n",
      "\n",
      "Epoch 00026: loss improved from 1.26971 to 1.25558, saving model to saved/weights-026.hdf5\n",
      "Epoch 27/200\n",
      "167397/167397 [==============================] - 193s 1ms/step - loss: 1.2433\n",
      "\n",
      "Epoch 00027: loss improved from 1.25558 to 1.24330, saving model to saved/weights-027.hdf5\n",
      "Epoch 28/200\n",
      "167397/167397 [==============================] - 193s 1ms/step - loss: 1.2299\n",
      "\n",
      "Epoch 00028: loss improved from 1.24330 to 1.22990, saving model to saved/weights-028.hdf5\n",
      "Epoch 29/200\n",
      "167397/167397 [==============================] - 193s 1ms/step - loss: 1.2182\n",
      "\n",
      "Epoch 00029: loss improved from 1.22990 to 1.21822, saving model to saved/weights-029.hdf5\n",
      "Epoch 30/200\n",
      "167397/167397 [==============================] - 193s 1ms/step - loss: 1.2077\n",
      "\n",
      "Epoch 00030: loss improved from 1.21822 to 1.20773, saving model to saved/weights-030.hdf5\n",
      "Epoch 31/200\n",
      "167397/167397 [==============================] - 193s 1ms/step - loss: 1.1978\n",
      "\n",
      "Epoch 00031: loss improved from 1.20773 to 1.19778, saving model to saved/weights-031.hdf5\n",
      "Epoch 32/200\n",
      "167397/167397 [==============================] - 193s 1ms/step - loss: 1.1886\n",
      "\n",
      "Epoch 00032: loss improved from 1.19778 to 1.18864, saving model to saved/weights-032.hdf5\n",
      "Epoch 33/200\n",
      "167397/167397 [==============================] - 194s 1ms/step - loss: 1.1753\n",
      "\n",
      "Epoch 00033: loss improved from 1.18864 to 1.17527, saving model to saved/weights-033.hdf5\n",
      "Epoch 34/200\n",
      "167397/167397 [==============================] - 193s 1ms/step - loss: 1.1670\n",
      "\n",
      "Epoch 00034: loss improved from 1.17527 to 1.16703, saving model to saved/weights-034.hdf5\n",
      "Epoch 35/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167397/167397 [==============================] - 184s 1ms/step - loss: 1.1590\n",
      "\n",
      "Epoch 00035: loss improved from 1.16703 to 1.15903, saving model to saved/weights-035.hdf5\n",
      "Epoch 36/200\n",
      "167397/167397 [==============================] - 184s 1ms/step - loss: 1.1520\n",
      "\n",
      "Epoch 00036: loss improved from 1.15903 to 1.15202, saving model to saved/weights-036.hdf5\n",
      "Epoch 37/200\n",
      "167397/167397 [==============================] - 185s 1ms/step - loss: 1.1475\n",
      "\n",
      "Epoch 00037: loss improved from 1.15202 to 1.14747, saving model to saved/weights-037.hdf5\n",
      "Epoch 38/200\n",
      "167397/167397 [==============================] - 185s 1ms/step - loss: 1.1369\n",
      "\n",
      "Epoch 00038: loss improved from 1.14747 to 1.13689, saving model to saved/weights-038.hdf5\n",
      "Epoch 39/200\n",
      "167397/167397 [==============================] - 185s 1ms/step - loss: 1.1301\n",
      "\n",
      "Epoch 00039: loss improved from 1.13689 to 1.13009, saving model to saved/weights-039.hdf5\n",
      "Epoch 40/200\n",
      "167397/167397 [==============================] - 185s 1ms/step - loss: 1.1220\n",
      "\n",
      "Epoch 00040: loss improved from 1.13009 to 1.12203, saving model to saved/weights-040.hdf5\n",
      "Epoch 41/200\n",
      "167397/167397 [==============================] - 186s 1ms/step - loss: 1.1164\n",
      "\n",
      "Epoch 00041: loss improved from 1.12203 to 1.11638, saving model to saved/weights-041.hdf5\n",
      "Epoch 42/200\n",
      "167397/167397 [==============================] - 185s 1ms/step - loss: 1.1098\n",
      "\n",
      "Epoch 00042: loss improved from 1.11638 to 1.10976, saving model to saved/weights-042.hdf5\n",
      "Epoch 43/200\n",
      "167397/167397 [==============================] - 186s 1ms/step - loss: 1.1041\n",
      "\n",
      "Epoch 00043: loss improved from 1.10976 to 1.10411, saving model to saved/weights-043.hdf5\n",
      "Epoch 44/200\n",
      "147136/167397 [=========================>....] - ETA: 22s - loss: 1.0890"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e636d9b3f45c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train model\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "random.seed(43)\n",
    "K = 100 # number of poems to train on\n",
    "WINDOW_SIZE = 30\n",
    "LAYER_SIZE = 160\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def get_wiki_text(title):\n",
    "    response = requests.get(\n",
    "        'https://en.wikipedia.org/w/api.php',\n",
    "        params={\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            'prop': 'extracts',\n",
    "            'titles': title,\n",
    "            'redirects': True\n",
    "         }\n",
    "    ).json()\n",
    "\n",
    "    text = ''\n",
    "    for value in response['query']['pages'].values(): text += value['extract']\n",
    "\n",
    "    text = TAG_RE.sub('', text)\n",
    "    return text\n",
    "\n",
    "def get_poems(k=10):\n",
    "    for path in random.sample(glob.glob('data/*/*.json'), k = k):\n",
    "        with open(path) as fi:\n",
    "            poem = \"\\n\".join(json.loads(fi.read())['text']).lower()\n",
    "            yield poem\n",
    "\n",
    "def get_alphabet(text):\n",
    "    return sorted(set(text))\n",
    "\n",
    "poems = list(get_poems(k=K))\n",
    "print(poems[0])\n",
    "\n",
    "# Mix poetry and Maersk\n",
    "maersk = get_wiki_text('Maersk').lower()\n",
    "poems.append(maersk)\n",
    "\n",
    "# Get alphabet\n",
    "poems_joined = ''.join(poems)\n",
    "alphabet = get_alphabet(poems_joined)\n",
    "print(alphabet)\n",
    "\n",
    "n_chars = len(poems_joined)\n",
    "n_distinct = len(alphabet)\n",
    "\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total distinct: \", n_distinct)\n",
    "\n",
    "int_to_char = dict([(i, c) for i, c in enumerate(alphabet)])\n",
    "char_to_oh = dict([(c, np.identity(n_distinct)[i: i+1][0]) for i, c in enumerate(alphabet)])\n",
    "\n",
    "# Create one-hot-encoded training data\n",
    "data_X = []\n",
    "data_y = []\n",
    "for poem in poems:\n",
    "    for i in range(0, len(poem) - WINDOW_SIZE, 1): \n",
    "        seq_in = [char_to_oh[c] for c in poem[i: i + WINDOW_SIZE]]\n",
    "        seq_out = char_to_oh[poem[i+WINDOW_SIZE]]\n",
    "        data_X.append(seq_in)\n",
    "        data_y.append(seq_out)\n",
    "    \n",
    "n_patterns = len(data_X)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "\n",
    "X = np.reshape(data_X, (n_patterns, WINDOW_SIZE, n_distinct))\n",
    "y = np.reshape(data_y, (n_patterns, n_distinct))\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(LSTM(LAYER_SIZE, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(LSTM(LAYER_SIZE))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Save model\n",
    "model_json = model.to_json()\n",
    "with open('saved/model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# define the checkpoint\n",
    "#filepath = 'saved/weights-{epoch:02d}-{loss:.4f}.hdf5'\n",
    "filepath = 'saved/weights-{epoch:03d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Train\n",
    "model.fit(X, y, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 30, 160)           156160    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 30, 160)           0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 160)               205440    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 83)                13363     \n",
      "=================================================================\n",
      "Total params: 374,963\n",
      "Trainable params: 374,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Seed:\n",
      " \n",
      "i must go down to the seas again, to the lonely sea and the sky,\n",
      "and all i ask is a tall ship and a star to steer her by;\n",
      "and the wheel’s kick and the wind’s song and the white sail’s shaking,\n",
      "and a grey mist on the sea’s face, and a grey dawn breaking.\n",
      "\n",
      "\n",
      "Generating 512-character poem after 1 iteration(s):\n",
      "\n",
      "e the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard the shard \n",
      "<EOP>\n",
      "\n",
      "Generating 512-character poem after 11 iteration(s):\n",
      "\n",
      "ainst the shadows of the company sea the shadow of the company sea the shadow of the company sea the shadow of the company sea the shadow of the company sea the shadow of the company sea the shadow of the company sea the shadow of the company sea the shadow of the company sea the shadow of the company sea the shadow of the company sea the shadow of the company sea the shadow of the company sea the shadow of the company sea the shadow of the company sea the shadow of the company sea the shadow of the company\n",
      "<EOP>\n",
      "\n",
      "Generating 512-character poem after 21 iteration(s):\n",
      "\n",
      "ainst the shadow of the secret her hands\n",
      "and the stars of the hand of a parple of the world and the shadow of the spring\n",
      "in the shadow of the shadows of the shadows of the chairs\n",
      "and the shadow of the shadows of the shadows of the chairs\n",
      "and the shadow of the shadows of the shadows of the chairs\n",
      "and the shadow of the shadows of the shadows of the chairs\n",
      "and the shadow of the shadows of the shadows of the chairs\n",
      "and the shadow of the shadows of the shadows of the chairs\n",
      "and the shadow of the shadows of the s\n",
      "<EOP>\n",
      "\n",
      "Generating 512-character poem after 31 iteration(s):\n",
      "\n",
      "ainst the secret contract of the ship and the sharts,\n",
      "the sense that stor like a belioth.\n",
      "the sea share and his graving and clouds.\n",
      "the clouds are the same ssigh’d said.\n",
      "what is my life span i stand their secret moved\n",
      "in the house in the desert,\n",
      "and the fire desires and still around the charges, and the book of the dead,\n",
      "the stars of the company is bared to make the shadow of the sun.\n",
      "the company of chard of the cold are lives and colorn container ship and colors of the children and the stand and stones and\n",
      "<EOP>\n",
      "\n",
      "Generating 512-character poem after 41 iteration(s):\n",
      "\n",
      "ainst the charges, and the contract for a light.\n",
      "i will not distant the sea barn\n",
      "sodeen the lighted by the seased to the sea.”\n",
      "and then a barned god shore;\n",
      "and the life in the world. which is you can to be a doing\n",
      "which comes from a little sun-ear\n",
      "with the shadows into the great cheme.\n",
      "when the little spend of the suns.\n",
      "the company production is the world. the company profile, and i am they are all the children and the container shipping and life in different in supply service centres the company $100 milli\n",
      "<EOP>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use model\n",
    "\"\"\"\n",
    "from keras.models import model_from_json\n",
    "\n",
    "GEN_LENGTH = 512\n",
    "DEFAULT_SEED = \\\n",
    "\"\"\"\n",
    "I must go down to the seas again, to the lonely sea and the sky,\n",
    "And all I ask is a tall ship and a star to steer her by;\n",
    "And the wheel’s kick and the wind’s song and the white sail’s shaking,\n",
    "And a grey mist on the sea’s face, and a grey dawn breaking.\n",
    "\"\"\".lower()\n",
    "\n",
    "DEFAULT_SEED_2 = \\\n",
    "\"\"\"\n",
    "if I go down tonight.\n",
    "let a seal be my friend.\n",
    "teach me to fish.\n",
    "and dance with the sea weeds.\n",
    "if only as a passive partner.\n",
    "arms to the right.\n",
    "legs to the left.\n",
    "head to the back.\n",
    "feet in the air.\"\"\".lower()\n",
    "\n",
    "def generate_poetry(seed_text, model, length=200):\n",
    "    # seed pattern\n",
    "    pattern = [char_to_oh[c] for c in seed_text[:WINDOW_SIZE]]\n",
    "\n",
    "    # generate characters\n",
    "    for i in range(length):\n",
    "        X_next = np.reshape(pattern, (1, WINDOW_SIZE, n_distinct))\n",
    "        prediction = model.predict(X_next, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        predicted_char = int_to_char[index]\n",
    "        padding = char_to_oh[predicted_char]\n",
    "        pattern.append(padding)\n",
    "        pattern = pattern[1:]\n",
    "        yield predicted_char\n",
    "\n",
    "# load model and weights from disk\n",
    "with open('saved/model.json', 'r') as json_file: \n",
    "    loaded_model_json = json_file.read()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.summary()\n",
    "    print()\n",
    "\n",
    "def load_weights(model, epoch):\n",
    "    weights = 'saved/weights-{epoch:03d}.hdf5'.format(epoch=epoch)\n",
    "    model.load_weights(weights)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "print('Seed:\\n', DEFAULT_SEED)\n",
    "print()\n",
    "\n",
    "epochs = sorted([int(re.findall(r'\\d+', path)[0]) for path in glob.glob('saved/weight*')])\n",
    "for epoch in epochs[0::10]:\n",
    "    # Set weights on model\n",
    "    load_weights(model, epoch)\n",
    "    # Generate poem\n",
    "    print('Generating {}-character poem after {} iteration(s):'.format(GEN_LENGTH, epoch))\n",
    "    print()\n",
    "    for c in generate_poetry(DEFAULT_SEED, model, length=GEN_LENGTH):\n",
    "        sys.stdout.write(c)\n",
    "    print()\n",
    "    print('<EOP>')        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fragment = 'the brides are'\n",
    "fragment in ''.join(poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " damarance\n",
      "the parts and the correnish propersion of the sea,\n",
      "the soul is the proper craps of the children and the children and the room in the desert of the country of the dead.\n",
      "the brides art the intricate wind a shining removed\n",
      "from the rook is the strings in the short of our life shall reves the tries, but my life span.\n",
      "to consummer that the shadow of the world.\n",
      "then the fire and survices of the country of the dead,\n",
      "the brides are stars of the country of the dead,\n",
      "the brides are stars of the country of the dead,\n",
      "the brides are stars of the country of the dead,\n",
      "the brides are stars of the country of the dead\n",
      "<EOP>\n"
     ]
    }
   ],
   "source": [
    "OLIVE_SEED = \\\n",
    "\"\"\"\n",
    "a ship my ship containers are nice\n",
    "I stuff my containers full of olives and mice\n",
    "the waves lay gently a craddle of money\n",
    "my future is golden from olives and honey\n",
    "\"\"\"\n",
    "\n",
    "BEST_EPOCH = 43\n",
    "#BEST_EPOCH = 41\n",
    "\n",
    "# load model and weights from disk\n",
    "with open('saved/model.json', 'r') as json_file: \n",
    "    loaded_model_json = json_file.read()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "\n",
    "load_weights(model, BEST_EPOCH)\n",
    "\n",
    "for c in generate_poetry(OLIVE_SEED, model, length=GEN_LENGTH+106):\n",
    "    sys.stdout.write(c)\n",
    "print()\n",
    "print('<EOP>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did the model spontaneously discover gaelic???\n",
    "\n",
    "\"\"\"\n",
    "maersk oil is an inde that the larine first seamed\n",
    "in the sand a could bu the coust skip and it with the everyt, \n",
    "and the she’s flag to the loved of the where a sand the poir,\n",
    "you beckin me them, the she loved chind it ups.\n",
    "they thought they had dead,\n",
    "a lood and herdy\n",
    "and the way it all the streek in they\n",
    "\n",
    "-- Poet = (2 layers, 128 neuron, 25 poems, 148 epochs)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "by the quait in the african by ever nerver market for the sapping come, \n",
    "the streek boteer in chilling to her the rood ass.\n",
    "but heavents in 1993, he earth he dainter speas, \n",
    "danglush, bird\n",
    "\n",
    "-- Poet = (2 layers, 128 neuron, 25 poems, 148 epochs)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "in a starts nime suppey the ever neart,\n",
    "i pare the donnest,\n",
    "a righ wo reavly he which a stanling.\n",
    "whether when it singes in the sidan\n",
    "\n",
    "-- Poet = (2 layers, 128 neuron, 25 poems, 148 epochs)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
