{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "\n",
    "random.seed(43)\n",
    "K = 100 # number of poems to train on\n",
    "WINDOW_SIZE = 30\n",
    "LAYER_SIZE = 160\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "LOAD_DIR = 'saved2' # location of pretrained models (model.json and weights-*.hdf5)\n",
    "SAVE_DIR = 'saved'  # location where new model and weights are stored during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare data\n",
    "\"\"\"\n",
    "\n",
    "def get_wiki_text(title):\n",
    "    response = requests.get(\n",
    "        'https://en.wikipedia.org/w/api.php',\n",
    "        params={\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            'prop': 'extracts',\n",
    "            'titles': title,\n",
    "            'redirects': True\n",
    "         }\n",
    "    ).json()\n",
    "\n",
    "    text = ''\n",
    "    for value in response['query']['pages'].values(): text += value['extract']\n",
    "\n",
    "    text = TAG_RE.sub('', text)\n",
    "    return text\n",
    "\n",
    "def get_poems(k=10):\n",
    "    for path in random.sample(glob.glob('data/*/*.json'), k = k):\n",
    "        with open(path) as fi:\n",
    "            poem = \"\\n\".join(json.loads(fi.read())['text']).lower()\n",
    "            yield poem\n",
    "\n",
    "def get_alphabet(text):\n",
    "    return sorted(set(text))\n",
    "\n",
    "poems = list(get_poems(k=K))\n",
    "print(poems[0])\n",
    "\n",
    "# Mix poetry and Maersk\n",
    "maersk = get_wiki_text('Maersk').lower()\n",
    "poems.append(maersk)\n",
    "\n",
    "# Get alphabet\n",
    "poems_joined = ''.join(poems)\n",
    "alphabet = get_alphabet(poems_joined)\n",
    "print(alphabet)\n",
    "\n",
    "n_chars = len(poems_joined)\n",
    "n_distinct = len(alphabet)\n",
    "\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total distinct: \", n_distinct)\n",
    "\n",
    "int_to_char = dict([(i, c) for i, c in enumerate(alphabet)])\n",
    "char_to_oh = dict([(c, np.identity(n_distinct)[i: i+1][0]) for i, c in enumerate(alphabet)])\n",
    "\n",
    "# Create one-hot-encoded training data\n",
    "data_X = []\n",
    "data_y = []\n",
    "for poem in poems:\n",
    "    for i in range(0, len(poem) - WINDOW_SIZE, 1): \n",
    "        seq_in = [char_to_oh[c] for c in poem[i: i + WINDOW_SIZE]]\n",
    "        seq_out = char_to_oh[poem[i+WINDOW_SIZE]]\n",
    "        data_X.append(seq_in)\n",
    "        data_y.append(seq_out)\n",
    "    \n",
    "n_patterns = len(data_X)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "\n",
    "X = np.reshape(data_X, (n_patterns, WINDOW_SIZE, n_distinct))\n",
    "y = np.reshape(data_y, (n_patterns, n_distinct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train\n",
    "\"\"\"\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(LSTM(LAYER_SIZE, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(LSTM(LAYER_SIZE))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Save model\n",
    "model_json = model.to_json()\n",
    "with open('saved/model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# define the checkpoint\n",
    "#filepath = 'saved/weights-{epoch:02d}-{loss:.4f}.hdf5'\n",
    "filepath = SAVE_DIR+'/weights-{epoch:03d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Train\n",
    "model.fit(X, y, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use model\n",
    "\"\"\"\n",
    "GEN_LENGTH = 512\n",
    "DEFAULT_SEED = \\\n",
    "\"\"\"\n",
    "I must go down to the seas again, to the lonely sea and the sky,\n",
    "And all I ask is a tall ship and a star to steer her by;\n",
    "And the wheel’s kick and the wind’s song and the white sail’s shaking,\n",
    "And a grey mist on the sea’s face, and a grey dawn breaking.\n",
    "\"\"\".lower()\n",
    "\n",
    "def generate_poetry(seed_text, model, length=200):\n",
    "    # seed pattern\n",
    "    pattern = [char_to_oh[c] for c in seed_text[:WINDOW_SIZE]]\n",
    "\n",
    "    # generate characters\n",
    "    for i in range(length):\n",
    "        X_next = np.reshape(pattern, (1, WINDOW_SIZE, n_distinct))\n",
    "        prediction = model.predict(X_next, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        predicted_char = int_to_char[index]\n",
    "        padding = char_to_oh[predicted_char]\n",
    "        pattern.append(padding)\n",
    "        pattern = pattern[1:]\n",
    "        yield predicted_char\n",
    "\n",
    "def list_epochs(directory):\n",
    "    paths = glob.glob('{}/weight*'.format(directory))\n",
    "    return sorted([int(re.findall(r'weights-(\\d+)', path)[0]) for path in paths])\n",
    "        \n",
    "def load_model(directory, epoch=None):\n",
    "    \"\"\"\n",
    "    Load model from model.json and weights from weights-*.hdf5. \n",
    "    If epoch is specified, then load corresponding weights, \n",
    "    else latest epoch in directory.\n",
    "    \"\"\"\n",
    "    with open('{}/model.json'.format(directory), 'r') as json_file: \n",
    "        loaded_model_json = json_file.read()\n",
    "        model = model_from_json(loaded_model_json)\n",
    "    if epoch is None:\n",
    "        # Load weights with highest epoch number\n",
    "        epoch = max(list_epochs(directory))\n",
    "    weights = '{directory}/weights-{epoch:03d}.hdf5'.format(directory=directory, epoch=epoch)\n",
    "    model.load_weights(weights)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "        \n",
    "model = load_model(LOAD_DIR)\n",
    "model.summary()\n",
    "\n",
    "print('Seed:\\n', DEFAULT_SEED)\n",
    "print()\n",
    "\n",
    "for epoch in list_epochs(LOAD_DIR)[0::10]:\n",
    "    # Set weights on model\n",
    "    model = load_model(LOAD_DIR, epoch=epoch)\n",
    "    # Generate poem\n",
    "    print('Generating {}-character poem after {} iteration(s):'.format(GEN_LENGTH, epoch))\n",
    "    print()\n",
    "    for c in generate_poetry(DEFAULT_SEED, model, length=GEN_LENGTH):\n",
    "        sys.stdout.write(c)\n",
    "    print()\n",
    "    print('<EOP>')        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLIVE_SEED = \\\n",
    "\"\"\"\n",
    "a ship my ship containers are nice\n",
    "I stuff my containers full of olives and mice\n",
    "the waves lay gently a craddle of money\n",
    "my future is golden from olives and honey\n",
    "\"\"\"\n",
    "\n",
    "BEST_EPOCH = 43\n",
    "\n",
    "model = load_model(LOAD_DIR, epoch=BEST_EPOCH)\n",
    "\n",
    "for c in generate_poetry(OLIVE_SEED, model, length=GEN_LENGTH+106):\n",
    "    sys.stdout.write(c)\n",
    "print()\n",
    "print('<EOP>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragment = 'the brides are'\n",
    "fragment in ''.join(poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
